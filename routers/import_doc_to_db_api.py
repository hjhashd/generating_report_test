import logging
import os
import shutil
import uuid
import zipfile
import threading
from fastapi import APIRouter, UploadFile, File, Form, BackgroundTasks, Depends, HTTPException, status
from utils.zzp.import_doc_to_db import process_document, scan_docx_structure
from routers.dependencies import require_user, CurrentUser

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

router = APIRouter()

# ç®€å•çš„å†…å­˜ä»»åŠ¡çŠ¶æ€å­˜å‚¨ï¼ˆç”Ÿäº§ç¯å¢ƒå»ºè®®ä½¿ç”¨ Redis æˆ–æ•°æ®åº“ï¼‰
task_status_store = {}

# å¹¶å‘æ§åˆ¶ï¼šé™åˆ¶åŒæ—¶è¿›è¡Œçš„æ–‡æ¡£å¤„ç†ä»»åŠ¡æ•°é‡
# æœåŠ¡å™¨é…ç½®ï¼š251GB å†…å­˜ï¼Œ80 æ ¸ CPUã€‚
# å³ä½¿é…ç½®å¾ˆé«˜ï¼Œä¸ºäº†é˜²æ­¢æç«¯å¹¶å‘å¯¼è‡´ OOMï¼Œè®¾ç½®ä¸€ä¸ªå®‰å…¨ä¸Šé™ã€‚
# å‡è®¾æ¯ä¸ªå¤§æ–‡ä»¶å¤„ç†æ¶ˆè€— 2-4GB å†…å­˜ï¼Œ20 ä¸ªå¹¶å‘çº¦å ç”¨ 40-80GBï¼Œéå¸¸å®‰å…¨ã€‚
MAX_CONCURRENT_TASKS = 20
task_semaphore = threading.Semaphore(MAX_CONCURRENT_TASKS)

def background_process_wrapper(task_id: str, type_name: str, report_name: str, file_path: str, user_id: int):
    """åå°ä»»åŠ¡åŒ…è£…å™¨ï¼Œç”¨äºæ›´æ–°ä»»åŠ¡çŠ¶æ€å¹¶æ‰§è¡Œå¤„ç†"""
    acquired = False
    
    # å®šä¹‰è¿›åº¦å›è°ƒå‡½æ•°
    def update_progress(percent: int, msg: str):
        if task_id in task_status_store:
            task_status_store[task_id]["progress"] = percent
            task_status_store[task_id]["message"] = msg

    try:
        # å°è¯•è·å–ä¿¡å·é‡ï¼Œå¦‚æœæ»¡äº†åˆ™ç­‰å¾…
        logger.info(f"â³ [ä»»åŠ¡ç­‰å¾…] ID: {task_id} æ­£åœ¨ç­‰å¾…æ‰§è¡Œæ§½ä½ (å½“å‰å¹¶å‘é™åˆ¶: {MAX_CONCURRENT_TASKS})...")
        if task_id in task_status_store:
            task_status_store[task_id].update({"status": "queued", "message": "æ­£åœ¨æ’é˜Ÿç­‰å¾…å¤„ç†èµ„æº...", "progress": 5})
        
        task_semaphore.acquire()
        acquired = True
        
        logger.info(f"â–¶ï¸ [ä»»åŠ¡å¼€å§‹] ID: {task_id} è·å–åˆ°æ‰§è¡Œæ§½ä½")
        if task_id in task_status_store:
            task_status_store[task_id].update({"status": "processing", "message": "æ­£åœ¨åå°å¤„ç†ä¸­...", "progress": 10})

        # 1. åå°æ‰«ææ–‡æ¡£ç»“æ„ (ä¼˜åŒ–å“åº”é€Ÿåº¦)
        try:
            logger.info(f"ğŸ“‘ [åå°ä»»åŠ¡] ID: {task_id} å¼€å§‹æ‰«ææ–‡æ¡£ç»“æ„...")
            doc_structure = scan_docx_structure(file_path)
            # æ›´æ–°çŠ¶æ€ä¸­çš„ç»“æ„ä¿¡æ¯ï¼Œä¾›å‰ç«¯è½®è¯¢è·å–
            if task_id in task_status_store:
                task_status_store[task_id]["structure"] = doc_structure
                task_status_store[task_id]["progress"] = 20 # æ›´æ–°è¿›åº¦
            logger.info(f"ğŸ“‘ [åå°ä»»åŠ¡] ID: {task_id} ç»“æ„æ‰«æå®Œæˆï¼Œå…± {len(doc_structure)} ç« èŠ‚")
        except Exception as e:
            logger.warning(f"âš ï¸ [åå°ä»»åŠ¡] ID: {task_id} ç»“æ„æ‰«æå¤±è´¥: {e}")
        
        # è°ƒç”¨æ ¸å¿ƒå¤„ç†é€»è¾‘ï¼Œä¼ å…¥å›è°ƒå’Œ user_id
        is_success, result_msg = process_document(type_name, report_name, file_path, progress_callback=update_progress, user_id=user_id)
        
        if is_success:
            if task_id in task_status_store:
                task_status_store[task_id].update({
                    "status": "success", 
                    "message": result_msg, 
                    "progress": 100,
                    "result": {
                        "report_generation_status": 0,
                        "report_generation_condition": result_msg,
                        "reportName": report_name,
                        "reportType": type_name,
                        "task_id": task_id
                    }
                })
            logger.info(f"âœ… [å¼‚æ­¥ä»»åŠ¡å®Œæˆ] ID: {task_id} {result_msg}")
        else:
            if task_id in task_status_store:
                task_status_store[task_id].update({
                    "status": "failed", 
                    "message": f"å¯¼å…¥å¤±è´¥ï¼š{result_msg}", 
                    "progress": 100
                })
            logger.warning(f"âš ï¸ [å¼‚æ­¥ä»»åŠ¡å¤±è´¥] ID: {task_id} {result_msg}")
            
    except Exception as e:
        logger.error(f"âŒ [å¼‚æ­¥ä»»åŠ¡å¼‚å¸¸] ID: {task_id} {e}", exc_info=True)
        
        error_msg = str(e)
        user_friendly_msg = f"ç³»ç»Ÿå¤„ç†å¼‚å¸¸: {error_msg}"
        error_code = "UNKNOWN_ERROR"
        
        if "There is no item named" in error_msg and "in the archive" in error_msg:
             user_friendly_msg = "æ–‡ä»¶ä¼¼ä¹å·²æŸåï¼Œå†…éƒ¨ç»“æ„ç¼ºå¤±ï¼Œè¯·å°è¯•ä¿®å¤æ–‡æ¡£æˆ–é‡æ–°ä¿å­˜åå†ä¸Šä¼ ã€‚"
             error_code = "DOCX_CORRUPTED"
             logger.info(f"â„¹ï¸ [é”™è¯¯ä¿¡æ¯è½¬æ¢] å°†åŸå§‹é”™è¯¯è½¬æ¢ä¸ºå‹å¥½æç¤º: {user_friendly_msg}")
        elif "BadZipFile" in error_msg or "zipfile" in str(type(e)).lower():
             user_friendly_msg = "æ–‡ä»¶æ ¼å¼é”™è¯¯æˆ–å·²æŸåï¼Œæ— æ³•è§£æã€‚è¯·ç¡®è®¤æ–‡ä»¶æ˜¯å¦ä¸ºæœ‰æ•ˆçš„ .docx æ–‡æ¡£ã€‚"
             error_code = "DOCX_CORRUPTED"
        
        if task_id in task_status_store:
            task_status_store[task_id].update({
                "status": "error", 
                "message": user_friendly_msg, 
                "progress": 100,
                "error_code": error_code
            })
    finally:
        if acquired:
            task_semaphore.release()
            logger.info(f"â¹ï¸ [ä»»åŠ¡é‡Šæ”¾] ID: {task_id} é‡Šæ”¾æ‰§è¡Œæ§½ä½")
            
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(file_path):
            try:
                os.remove(file_path)
                logger.info(f"ğŸ§¹ å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {file_path}")
            except Exception as cleanup_error:
                logger.error(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {cleanup_error}")

@router.get("/check_import_status/{task_id}")
def check_import_status(task_id: str, current_user: CurrentUser = Depends(require_user)):
    """æŸ¥è¯¢å¯¼å…¥ä»»åŠ¡çŠ¶æ€ (éœ€ç™»å½•ï¼Œä¸”åªèƒ½æŸ¥è‡ªå·±çš„ä»»åŠ¡)"""
    status_info = task_status_store.get(task_id)
    if not status_info:
        return {"status": "unknown", "message": "ä»»åŠ¡ä¸å­˜åœ¨"}
    
    # æƒé™æ ¡éªŒï¼šåªèƒ½æŸ¥çœ‹å±äºè‡ªå·±çš„ä»»åŠ¡
    owner_id = status_info.get("owner_user_id")
    current_user_id = current_user.id
    if owner_id is not None and str(owner_id) != str(current_user_id):
        # ä¸ºäº†å®‰å…¨ï¼Œè¿™é‡Œå¯ä»¥è¿”å› 404ï¼Œå‡è£…ä»»åŠ¡ä¸å­˜åœ¨
        logger.warning(f"âš ï¸ [è¶Šæƒè®¿é—®] User {current_user_id} å°è¯•æŸ¥çœ‹ User {owner_id} çš„ä»»åŠ¡ {task_id}")
        return {"status": "unknown", "message": "ä»»åŠ¡ä¸å­˜åœ¨"}
        
    return status_info

@router.post("/Import_Doc/")
async def Import_Doc_endpoint(  # æ”¹ä¸ºasync
    background_tasks: BackgroundTasks,
    task_id: str = Form(...),
    status: int = Form(...),
    agentUserId: int = Form(...),
    type_name: str = Form(...),
    report_name: str = Form(...),
    file: UploadFile = File(...),
    current_user: CurrentUser = Depends(require_user)
):
    # ä¼˜å…ˆä½¿ç”¨ Token ä¸­çš„ç”¨æˆ· ID
    user_id = current_user.id
    logger.info(f'ğŸš€ [ä»»åŠ¡æ¥æ”¶] ID: {task_id}, User: {user_id} (Claimed: {agentUserId}), æŠ¥å‘Š: {report_name}, ç±»å‹: {type_name}, æ¨¡å¼: å¼‚æ­¥å¤„ç†')

    # 1. è·¯å¾„å‡†å¤‡
    current_dir = os.getcwd()
    temp_dir = os.path.join(current_dir, "temp_uploads")
    if not os.path.exists(temp_dir):
        os.makedirs(temp_dir)
    
    # 2. æå–å¹¶éªŒè¯æ–‡ä»¶æ‰©å±•å
    original_filename = file.filename
    file_ext = os.path.splitext(original_filename)[1].lower()
    
    # 3. ä¸¥æ ¼æ ¡éªŒæ–‡ä»¶æ ¼å¼ï¼Œåªæ¥å—.docx
    if file_ext != '.docx':
        return {
            "report_generation_status": 1,
            "report_generation_condition": "ç³»ç»Ÿä»…æ”¯æŒæ ‡å‡† OpenXML æ ¼å¼çš„ .docx æ–‡æ¡£ï¼Œè¯·å‹¿ä½¿ç”¨æ—§ç‰ˆ .doc æˆ–æ‰‹åŠ¨ä¿®æ”¹åç¼€åã€‚",
            "task_id": task_id,
            "error_code": "UNSUPPORTED_FILE_FORMAT"
        }

    unique_filename = f"{uuid.uuid4()}{file_ext}"
    temp_file_path = os.path.join(temp_dir, unique_filename)

    try:
        # 4. é‡ç½®æ–‡ä»¶æŒ‡é’ˆï¼Œç¡®ä¿ä»å¼€å¤´è¯»å–
        await file.seek(0)

        # 5. ä¿å­˜æ–‡ä»¶ (ä½¿ç”¨åˆ†å—å†™å…¥ä»¥æ”¯æŒå¤§æ–‡ä»¶)
        with open(temp_file_path, "wb") as buffer:
            # ä½¿ç”¨åˆ†å—å†™å…¥é¿å…å¤§æ–‡ä»¶å†…å­˜æº¢å‡º
            chunk_size = 8192
            while True:
                chunk = await file.read(chunk_size)
                if not chunk:
                    break
                buffer.write(chunk)
        
        # 6. å¼ºåˆ¶å°†æ•°æ®åˆ·å…¥ç£ç›˜ï¼ˆè§£å†³IOç«äº‰é—®é¢˜ï¼‰
        with open(temp_file_path, "ab") as buffer:
            buffer.flush()
            os.fsync(buffer.fileno())
        
        file_size = os.path.getsize(temp_file_path)
        logger.info(f"ğŸ“‚ æ–‡ä»¶å·²æ¥æ”¶: {temp_file_path}, å¤§å°: {file_size / 1024 / 1024:.2f} MB")

        if file_size == 0:
            os.remove(temp_file_path)
            return {"report_generation_status": 1, "report_generation_condition": "æ–‡ä»¶å¤§å°ä¸º0", "task_id": task_id}
        
        # 7. ä¸¥æ ¼çš„æ–‡ä»¶æ ¼å¼æ ¡éªŒ - åœ¨å¼ºåˆ¶åˆ·ç›˜åè¿›è¡Œ
        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆçš„ Zip æ–‡ä»¶ (docx æœ¬è´¨æ˜¯ zip)
        if not zipfile.is_zipfile(temp_file_path):
             logger.warning(f"âš ï¸ æ–‡ä»¶æ ¼å¼æ ¡éªŒå¤±è´¥: {temp_file_path} ä¸æ˜¯æœ‰æ•ˆçš„ zip/docx")
             os.remove(temp_file_path)
             return {
                 "report_generation_status": 1,
                 "report_generation_condition": "æ–‡ä»¶å·²æŸåæˆ–ä¸æ˜¯æœ‰æ•ˆçš„ Word (.docx) æ–‡æ¡£",
                 "task_id": task_id,
                 "error_code": "DOCX_CORRUPTED_OR_INVALID"
             }
        
        # 8. è¿›ä¸€æ­¥éªŒè¯æ˜¯å¦åŒ…å«docxå¿…è¦ç»“æ„
        try:
            with zipfile.ZipFile(temp_file_path, 'r') as zip_file:
                # æ£€æŸ¥æ˜¯å¦å­˜åœ¨å¿…è¦çš„docxæ–‡ä»¶
                required_files = ['word/document.xml', '[Content_Types].xml', 'word/_rels/document.xml.rels']
                missing_files = [f for f in required_files if f not in zip_file.namelist()]
                
                if missing_files:
                    logger.warning(f"âš ï¸ docxæ–‡ä»¶ç¼ºå°‘å¿…è¦ç»„ä»¶: {missing_files}")
                    os.remove(temp_file_path)
                    return {
                        "report_generation_status": 1,
                        "report_generation_condition": "æ–‡æ¡£ç»“æ„ä¸å®Œæ•´ï¼Œå¯èƒ½å·²æŸå",
                        "task_id": task_id,
                        "error_code": "DOCX_STRUCTURE_INCOMPLETE"
                    }
        except zipfile.BadZipFile:
            logger.warning(f"âš ï¸ æ–‡ä»¶æ ¼å¼æ ¡éªŒå¤±è´¥: {temp_file_path} æ— æ³•æ‰“å¼€ä¸ºzipæ–‡ä»¶")
            os.remove(temp_file_path)
            return {
                "report_generation_status": 1,
                "report_generation_condition": "æ–‡ä»¶å·²æŸåæˆ–ä¸æ˜¯æœ‰æ•ˆçš„ Word (.docx) æ–‡æ¡£",
                "task_id": task_id,
                "error_code": "DOCX_CORRUPTED_OR_INVALID"
            }

        # 9. åˆå§‹åŒ–ä»»åŠ¡çŠ¶æ€ (è®°å½• owner_user_id)
        task_status_store[task_id] = {
            "status": "pending",
            "message": "å·²è¿›å…¥å¤„ç†é˜Ÿåˆ—",
            "progress": 0,
            "owner_user_id": user_id
        }

        # 10. æäº¤åå°ä»»åŠ¡ (ç«‹å³å“åº”å‰ç«¯)
        # ä¼ å…¥ user_id
        background_tasks.add_task(background_process_wrapper, task_id, type_name, report_name, temp_file_path, user_id)

        # 11. ç«‹å³è¿”å›
        return {
            "report_generation_status": 0,
            "report_generation_condition": "æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼Œæ­£åœ¨åå°å¤„ç†ä¸­ï¼Œè¯·é€šè¿‡ /check_import_status æŸ¥è¯¢è¿›åº¦",
            "status": status,
            "task_id": task_id,
            "mode": "async"
        }

    except Exception as e:
        logger.error(f"âŒ æ¥æ”¶æ–‡ä»¶å¼‚å¸¸: {e}", exc_info=True)
        if os.path.exists(temp_file_path):
            try:
                os.remove(temp_file_path)
            except:
                pass  # å¦‚æœæ¸…ç†å¤±è´¥ï¼Œå¿½ç•¥
        return {
            "report_generation_status": 1,
            "report_generation_condition": f"æ¥æ”¶å¼‚å¸¸: {str(e)}",
            "task_id": task_id
        }

@router.get("/health")
def health_check():
    """ç®€å•çš„å¥åº·æ£€æŸ¥"""
    return {"status": "healthy"}