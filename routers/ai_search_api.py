import logging
import json
import time
import traceback
from fastapi import APIRouter, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import List, Optional

# æ ¸å¿ƒé€»è¾‘æ–‡ä»¶
from utils.lyf.ai_search import Search_Chat_Generator_Stream
from utils.zzp.ai_generate_langchain import get_llm_config_by_id

logger = logging.getLogger(__name__)
router = APIRouter()

STREAM_HEADERS = {
    "Cache-Control": "no-cache",
    "Connection": "keep-alive",
    "Content-Type": "text/event-stream",
    "X-Accel-Buffering": "no" 
}

class SearchRequest(BaseModel):
    task_id: str
    user_query: str
    id: int
    model_name: str
    base_url: str
    api_key: str
    status: Optional[int] = 1
    agentUserId: Optional[int] = None

@router.post("/ai_search/stream")
async def ai_search_endpoint(req: SearchRequest):
    """
    ã€è”ç½‘æœç´¢ã€‘æµå¼æ¥å£ - å¢å¼ºæ—¥å¿—ç‰ˆ
    """
    # 0. å°è¯•ä»æ•°æ®åº“è¡¥å……é…ç½® (å¦‚æœè¯·æ±‚ä¸­çš„é…ç½®ä¸å®Œæ•´)
    model_name = req.model_name
    base_url = req.base_url
    api_key = req.api_key
    
    if req.id and (not model_name or not api_key or model_name.strip() == ""):
        logger.info(f"ğŸ” å°è¯•ä»æ•°æ®åº“è·å–æ¨¡å‹é…ç½® | ID: {req.id}")
        db_config = get_llm_config_by_id(req.id)
        if db_config:
            model_name = db_config.get("model_name", model_name).strip()
            base_url = db_config.get("base_url", base_url)
            api_key = db_config.get("api_key", api_key)
            logger.info(f"âœ… å·²ä»æ•°æ®åº“åŠ è½½é…ç½®: '{model_name}'")

    # 1. è®°å½•è¯·æ±‚è¿›å…¥çš„è¯¦ç»†å…ƒæ•°æ®
    start_time = time.time()
    log_context = {
        "task_id": req.task_id,
        "model": model_name,
        "query_len": len(req.user_query),
        "user_id": req.agentUserId
    }
    
    logger.info(f"ğŸš€ [AI Search] æ”¶åˆ°æ–°è¯·æ±‚ | Context: {json.dumps(log_context, ensure_ascii=False)}")
    logger.debug(f"ğŸ“ [AI Search] å®Œæ•´é—®é¢˜: {req.user_query}")

    # 2. åŒ…è£…ç”Ÿæˆå™¨ä»¥æ•è·æµå¼ä¼ è¾“ä¸­çš„å¼‚å¸¸
    async def wrapped_generator():
        try:
            # è®°å½•æµå¼€å§‹
            logger.info(f"ğŸŒŠ [AI Search] æµè¾“å‡ºå¼€å§‹ | TaskID: {req.task_id}")
            
            chunk_count = 0
            async for chunk in Search_Chat_Generator_Stream(
                user_query=req.user_query,
                model_name=model_name,
                base_url=base_url,
                api_key=api_key,
                task_id=req.task_id
            ):
                yield chunk
                chunk_count += 1
            
            # 3. è®°å½•æµæ­£å¸¸ç»“æŸ
            duration = round(time.time() - start_time, 2)
            logger.info(f"âœ… [AI Search] æµè¾“å‡ºå®Œæˆ | TaskID: {req.task_id} | æ€»è€—æ—¶: {duration}s | æ•°æ®å—æ•°é‡: {chunk_count}")

        except Exception as e:
            # 4. å…³é”®ï¼šæ•è·ç”Ÿæˆå™¨å†…éƒ¨çš„å¼‚å¸¸å¹¶è®°å½•å †æ ˆ
            duration = round(time.time() - start_time, 2)
            error_msg = traceback.format_exc()
            logger.error(f"âŒ [AI Search] æµè¾“å‡ºä¸­æ–­ | TaskID: {req.task_id} | è€—æ—¶: {duration}s | é”™è¯¯: {str(e)}\n{error_msg}")
            
            # å‘å‰ç«¯æ¨é€ä¸€ä¸ªç¬¦åˆ SSE æ ¼å¼çš„é”™è¯¯æ¶ˆæ¯
            # æ”¹ä¸º content å­—æ®µï¼Œç¡®ä¿å‰ç«¯èƒ½æ˜¾ç¤º
            error_payload = json.dumps({"content": f"\n\nâŒ [ç³»ç»Ÿé”™è¯¯] æ¥å£å¤„ç†ä¸­æ–­: {str(e)}", "task_id": req.task_id}, ensure_ascii=False)
            yield f"data: {error_payload}\n\n"

    try:
        return StreamingResponse(
            wrapped_generator(),
            media_type="text/event-stream",
            headers=STREAM_HEADERS
        )
    except Exception as e:
        # è¿™é‡Œæ•è·çš„æ˜¯åˆå§‹åŒ– StreamingResponse ä¹‹å‰çš„é”™è¯¯
        logger.error(f"ğŸš¨ [AI Search] æ¥å£å¯åŠ¨å¤±è´¥ | TaskID: {req.task_id} | Error: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/health")
def health_check():
    return {"status": "healthy"}