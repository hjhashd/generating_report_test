import os
import sys
import json
import re
import time
import logging
from typing import List, Optional, Dict, Any
from datetime import datetime

# æ•°æ®åº“ä¸åŠ å¯†ç›¸å…³
import pymysql
from sqlalchemy import create_engine, text
from urllib.parse import quote_plus
from cryptography.fernet import Fernet

# LangChain ç›¸å…³åº“
from langchain_ollama import ChatOllama
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from utils.chat_session_manager import ChatSessionManager

# ==============================
# 0. åŸºç¡€é…ç½® & å¯†é’¥ç®¡ç†
# ==============================
# ç¡®ä¿å¯ä»¥å¼•å…¥åŒçº§æˆ–ä¸Šçº§æ¨¡å—
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if project_root not in sys.path:
    sys.path.append(project_root)
from zzp import sql_config as config

ENCRYPTION_KEY = b'8P_Gk9wz9qKj-4t8z9qKj-4t8z9qKj-4t8z9qKj-4t8=' 
cipher_suite = Fernet(ENCRYPTION_KEY)

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger("LangChainOptimizer")

# ==============================
# 1. æ•°æ®åº“å·¥å…·å‡½æ•°
# ==============================
def get_db_connection():
    encoded_password = quote_plus(config.password)
    db_url = f"mysql+pymysql://{config.username}:{encoded_password}@{config.host}:{config.port}/{config.database}"
    return create_engine(db_url)

def decrypt_text(encrypted_str):
    if not encrypted_str: return ""
    try:
        return cipher_suite.decrypt(encrypted_str.encode()).decode()
    except Exception:
        # Fallback: if it looks like a raw key (starts with "sk-"), return it directly
        if encrypted_str.startswith("sk-"):
            return encrypted_str
        return ""

def get_llm_config_by_id(model_id):
    """ä»æ•°æ®åº“è·å–æ¨¡å‹é…ç½®"""
    engine = get_db_connection()
    sql = text("SELECT llm_type, model_name, api_key, base_url FROM llm_config WHERE id = :id")
    try:
        with engine.connect() as conn:
            result = conn.execute(sql, {"id": model_id}).fetchone()
            if result:
                llm_type, model_name, encrypted_key, base_url = result
                api_key = decrypt_text(encrypted_key) if encrypted_key else ""
                return {
                    "llm_type": llm_type, "model_name": model_name,
                    "api_key": api_key, "base_url": base_url
                }
    except Exception as e:
        logger.error(f"è¯»å–é…ç½®å¤±è´¥: {e}")
    return None

# ==============================
# 2. ä¼šè¯ç®¡ç† (Redis + Memory)
# ==============================
# Initialize Manager with 'chat:optimize' session type
session_manager = ChatSessionManager(session_type="chat:optimize") 

# ==============================
# 3. å·¥å…·å‡½æ•°ï¼šPrompt æ„å»ºä¸ LLM åˆå§‹åŒ–
# ==============================

def build_optimization_prompt(text: str, requirements: List[str]) -> str:
    """
    æ ¹æ®å‰ç«¯ä¼ æ¥çš„ä¸­æ–‡éœ€æ±‚åˆ—è¡¨æ„å»º Prompt
    """
    if not requirements:
        req_str = "æ— ç‰¹æ®Šè¦æ±‚ï¼Œè¯·ä¼˜åŒ–è¯­è¨€ï¼Œä½¿å…¶æ›´åŠ é€šé¡ºã€ä¸“ä¸šã€‚"
    else:
        # å°†åˆ—è¡¨è½¬æ¢ä¸ºå¸¦åºå·çš„å­—ç¬¦ä¸²
        req_str = "\n".join([f"{i+1}. {req}" for i, req in enumerate(requirements)])

    # æ„å»ºæœ€ç»ˆæç¤ºè¯
    prompt = (
        f"è¯·æ ¹æ®ä»¥ä¸‹ã€æ¶¦è‰²è¦æ±‚ã€‘å¯¹ã€åŸå§‹å†…å®¹ã€‘è¿›è¡Œé‡å†™ã€‚\n\n"
        f"ã€æ¶¦è‰²è¦æ±‚ã€‘\n{req_str}\n\n"
        f"ã€åŸå§‹å†…å®¹ã€‘\n{text}\n\n"
        f"ã€è¾“å‡ºè¦æ±‚ã€‘\n"
        f"1. ç›´æ¥è¾“å‡ºæ¶¦è‰²åçš„æ­£æ–‡ï¼Œä¸è¦åŒ…å«â€œå¥½çš„â€ã€â€œä»¥ä¸‹æ˜¯ä¿®æ”¹åçš„å†…å®¹â€ç­‰å¯’æš„è¯­ã€‚\n"
        f"2. ä¿æŒåŸæ„ä¸å˜ï¼Œä½†æå‡è¡¨è¾¾è´¨é‡ã€‚"
    )
    return prompt

def init_llm_instance(model_id: int):
    """æ ¹æ® model_id åˆå§‹åŒ– LangChain LLM å®ä¾‹"""
    config_data = get_llm_config_by_id(model_id)
    if not config_data:
        # å…œåº•æ–¹æ¡ˆï¼šå¦‚æœæ‰¾ä¸åˆ°é…ç½®ï¼Œé»˜è®¤ä½¿ç”¨æœ¬åœ° Ollama
        logger.warning(f"âš ï¸ æœªæ‰¾åˆ° model_id={model_id} çš„é…ç½®ï¼Œä½¿ç”¨é»˜è®¤æœ¬åœ°æ¨¡å‹")
        return ChatOllama(
            model="llama3.2:3b",
            base_url="http://localhost:11434",
            temperature=0.3,
        )

    llm_type = config_data["llm_type"]
    model_name = config_data["model_name"]
    api_key = config_data["api_key"]
    base_url = config_data["base_url"]

    logger.info(f"ğŸš€ åˆå§‹åŒ–æ¨¡å‹: [{llm_type}] - {model_name}")
    
    if llm_type == "local":
        return ChatOllama(
            model=model_name,
            base_url=base_url if base_url else "http://localhost:11434",
            temperature=0.3,
            timeout=60, # å¢åŠ è¶…æ—¶è®¾ç½®
        )
    elif llm_type == "custom":
        return ChatOpenAI(
            api_key=api_key,
            base_url=base_url,
            model=model_name,
            temperature=0.3,
            streaming=True,
            timeout=60, # å¢åŠ è¶…æ—¶è®¾ç½®
        )
    else:
        # å…¼å®¹å…¶ä»– OpenAI æ ¼å¼
        return ChatOpenAI(
            api_key=api_key,
            base_url=base_url,
            model=model_name,
            temperature=0.3,
            streaming=True,
            timeout=60, # å¢åŠ è¶…æ—¶è®¾ç½®
        )

# ==============================
# 4. æ ¸å¿ƒæµå¼ç”Ÿæˆé€»è¾‘
# ==============================

def optimize_text_stream(text: str, requirements: List[str], model_id: int, task_id: str, user_id: int = None):
    """
    æµå¼æ¶¦è‰²ç”Ÿæˆå™¨
    :param text: åŸæ–‡
    :param requirements: å‰ç«¯éœ€æ±‚åˆ—è¡¨
    :param model_id: æ¨¡å‹ID
    :param task_id: ä¼šè¯IDï¼Œç”¨äºéš”ç¦»ä¸Šä¸‹æ–‡
    :param user_id: ç”¨æˆ·IDï¼Œç”¨äºæƒé™æ ¡éªŒ
    """
    global session_manager

    # 1. è·å–æˆ–åˆå§‹åŒ–å†å²è®°å½•
    current_history = session_manager.get_session(task_id)
    if not current_history:
        current_history = []
    
    # 2. æ„å»º System Prompt (å¦‚æœæ˜¯æ–°ä¼šè¯)
    #    å¦‚æœæ˜¯å¤šè½®å¯¹è¯ï¼Œæˆ‘ä»¬åªè¿½åŠ ç”¨æˆ·çš„åç»­æŒ‡ä»¤ï¼Œä¸å†é‡å¤å‘ System Prompt
    messages = []
    
    if len(current_history) == 0:
        system_content = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£æ¶¦è‰²ä¸“å®¶ï¼Œæ“…é•¿é€»è¾‘é‡ç»„ã€æœ¯è¯­æ ¡å¯¹å’Œå•†åŠ¡å†™ä½œã€‚è¯·ä¸¥æ ¼éµå¾ªç”¨æˆ·çš„æŒ‡ä»¤è¿›è¡Œä¿®æ”¹ã€‚"
        messages.append(SystemMessage(content=system_content))
    
    # 3. è½½å…¥å†å²è®°å½•
    messages.extend(current_history)

    # 4. æ„å»ºæœ¬æ¬¡è¯·æ±‚çš„ Prompt
    #    å¦‚æœæ˜¯ç¬¬ä¸€è½®ï¼Œæˆ‘ä»¬éœ€è¦æŠŠåŸæ–‡å’Œè¦æ±‚ç»„åˆèµ·æ¥
    #    å¦‚æœæ˜¯åç»­è½®æ¬¡ï¼ˆæ¯”å¦‚ç”¨æˆ·è¯´â€œå†æ”¹çŸ­ä¸€ç‚¹â€ï¼‰ï¼Œæˆ‘ä»¬ç›´æ¥æŠŠè¿™ä¸ªæŒ‡ä»¤å‘ç»™ AI
    if len(current_history) == 0:
        user_prompt_content = build_optimization_prompt(text, requirements)
    else:
        # å‡è®¾è¿™é‡Œ text æ˜¯ç”¨æˆ·çš„åç»­æŒ‡ä»¤ï¼Œæˆ–è€…æˆ‘ä»¬éœ€è¦é‡æ–°ç»„åˆ
        # ç®€å•èµ·è§ï¼Œæˆ‘ä»¬å‡è®¾æ¯æ¬¡è°ƒç”¨éƒ½æ˜¯ä¸€æ¬¡æ–°çš„æ¶¦è‰²è¯·æ±‚ï¼Œæˆ–è€…æ˜¯å¯¹ä¸Šä¸€æ¬¡çš„è¡¥å……
        # è¿™é‡Œæ¼”ç¤ºä½œä¸ºä¸€æ¬¡æ–°çš„å¼ºæŒ‡ä»¤
        user_prompt_content = build_optimization_prompt(text, requirements)

    messages.append(HumanMessage(content=user_prompt_content))

    # 5. æ‰§è¡Œæµå¼ç”Ÿæˆ
    try:
        llm = init_llm_instance(model_id)
        
        full_response_content = ""
        
        print(f"â³ (Task: {task_id}) æ­£åœ¨ç”Ÿæˆ...")

        # LangChain çš„ stream æ–¹æ³•
        for chunk in llm.stream(messages):
            text_chunk = chunk.content
            if text_chunk:
                full_response_content += text_chunk
                # æ„é€  SSE æ ¼å¼æ•°æ®
                yield f"data: {json.dumps({'content': text_chunk}, ensure_ascii=False)}\n\n"
        
        # å‘é€ç»“æŸæ ‡è®°
        yield "data: [DONE]\n\n"

        # 6. æ›´æ–°å†å²è®°å½• (å­˜å…¥ Redis/Memoryï¼Œæ”¯æŒå¤šè½®)
        current_history.append(HumanMessage(content=user_prompt_content))
        current_history.append(AIMessage(content=full_response_content))
        session_manager.update_session(task_id, current_history)
        logger.info(f"Task {task_id} å†å²è®°å½•å·²æ›´æ–°ï¼Œå½“å‰è½®æ•°: {len(current_history)//2}")

    except Exception as e:
        logger.error(f"Stream error: {e}")
        yield f"data: {json.dumps({'error': str(e)}, ensure_ascii=False)}\n\n"

# ==============================
# 5. ä¸»å‡½æ•°æµ‹è¯• (æ¨¡æ‹Ÿå‰ç«¯äº¤äº’)
# ==============================
async def main():
    print(f"ğŸš€ å¯åŠ¨ LangChain æµå¼æ¶¦è‰²æµ‹è¯•")
    
    # æ¨¡æ‹Ÿæ•°æ®
    test_task_id = "session_12345"
    test_model_id = 2  # å‡è®¾æ•°æ®åº“ä¸­æœ‰ ID ä¸º 1 çš„æ¨¡å‹
    input_text = """
    æˆ‘ä»¬è¿™ä¸ªAIé¡¹ç›®å…¶å®æŒºéš¾æçš„ï¼Œä¸»è¦æ˜¯æ•°æ®ä¸å¤ªè¡Œï¼Œè„æ•°æ®å¤ªå¤šäº†ã€‚
    ç„¶åé‚£ä¸ªç®—æ³•ä¹Ÿå°±æ˜¯ç”¨äº†ä¸ªå¼€æºçš„ï¼Œæ•ˆæœä¸€èˆ¬èˆ¬å§ã€‚
    å¦å¤–æœåŠ¡å™¨ç»å¸¸å´©ï¼Œå¹¶å‘ä¸€é«˜å°±æŒ‚ã€‚
    åæ­£ç°åœ¨å°±æ˜¯å…ˆæŠŠåŠŸèƒ½è·‘é€šï¼Œåé¢çš„ä»¥åå†è¯´ã€‚
    """
    frontend_requirements = [
        "ä¼˜åŒ–é€»è¾‘ç»“æ„",
        "ä¸“ä¸šæœ¯è¯­ä¼˜åŒ–",
        "æ€»-åˆ†-æ€» è¡¨è¿°"
    ]

    print("-" * 50)
    print("ğŸ“ åŸæ–‡å†…å®¹:")
    print(input_text.strip())
    print("-" * 50)
    print("â³ å¼€å§‹æµå¼æ¥æ”¶...")

    # è°ƒç”¨æµå¼ç”Ÿæˆå™¨
    # æ³¨æ„ï¼šè¿™é‡Œä¸æ˜¯ async è°ƒç”¨ï¼Œå› ä¸º generator æ˜¯åŒæ­¥çš„è¿­ä»£å™¨ï¼Œ
    # å¦‚æœæ˜¯åœ¨ FastAPI ä¸­ä½¿ç”¨ StreamingResponseï¼Œå®ƒä¼šåœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œã€‚
    generator = optimize_text_stream(input_text, frontend_requirements, test_model_id, test_task_id)
    
    full_content = ""
    
    for event in generator:
        # æ¨¡æ‹Ÿå‰ç«¯å¤„ç† SSE
        if "[DONE]" in event:
            print("\n\nâœ… æµå¼ä¼ è¾“ç»“æŸ")
            break
        
        try:
            # å»æ‰ "data: " å‰ç¼€
            if event.startswith("data: "):
                json_str = event[6:].strip()
                data = json.loads(json_str)
                
                if "content" in data:
                    chunk = data["content"]
                    print(chunk, end="", flush=True) # å®æ—¶æ‰“å°æ•ˆæœ
                    full_content += chunk
                
                if "error" in data:
                    print(f"\nâŒ Error: {data['error']}")
        except Exception as e:
            print(f"è§£æé”™è¯¯: {e}")

    # (å¯é€‰) ç®€å•çš„åå¤„ç†å±•ç¤ºï¼Œå¦‚æœéœ€è¦å»é™¤éæ–‡æœ¬å†…å®¹
    # Qwen é€šå¸¸ä¸éœ€è¦åƒ DeepSeek R1 é‚£æ ·å»é™¤ <think> æ ‡ç­¾
    print("-" * 50)
    print(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡: é•¿åº¦ {len(full_content)} å­—")

if __name__ == "__main__":
    import asyncio
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\næµ‹è¯•å·²ä¸­æ–­")