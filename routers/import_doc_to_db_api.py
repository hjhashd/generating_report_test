import logging
import os
import shutil
import uuid
import zipfile
import threading
import json
from fastapi import APIRouter, UploadFile, File, Form, BackgroundTasks, Depends, HTTPException, status
from utils.zzp.import_doc_to_db import process_document, scan_docx_structure
from routers.dependencies import require_user, CurrentUser
from utils.redis_client import get_redis_client

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

router = APIRouter()

class TaskStatusManager:
    """
    Manages task status persistence, switching between Redis and Memory based on configuration.
    Handles JSON serialization for complex fields.
    """
    def __init__(self):
        self.memory_store = {}
        self.redis_prefix = os.getenv("REDIS_PREFIX", "langextract")
        # Check specific feature flag first, then general enabled flag
        self.redis_enabled = (os.getenv("REDIS_TASK_STATUS_ENABLED", "0") == "1") and \
                             (os.getenv("REDIS_ENABLED", "0") == "1")
        self.ttl = 24 * 60 * 60  # 24 hours
        self.env = os.getenv("ENV", "dev")
        
        if self.redis_enabled:
            logger.info("ğŸš€ TaskStatusManager: Redis persistence ENABLED")
        else:
            logger.info("âš ï¸ TaskStatusManager: Using In-Memory Store (Redis disabled)")

    def _get_key(self, user_id, task_id):
        return f"{self.redis_prefix}:{self.env}:task:import:{user_id}:{task_id}"

    def _get_redis(self):
        try:
            client = get_redis_client()
            if client:
                return client
        except Exception as e:
            logger.error(f"Failed to get Redis client: {e}")
        return None

    def update(self, task_id, data, user_id):
        """
        Update task status.
        data: dict containing fields to update.
        user_id: required for key generation in Redis mode.
        """
        # 1. Try Redis if enabled
        if self.redis_enabled:
            client = self._get_redis()
            if client:
                try:
                    key = self._get_key(user_id, task_id)
                    # Prepare data for HSET (serialize complex types)
                    processed_data = {}
                    for k, v in data.items():
                        if isinstance(v, (dict, list)):
                            processed_data[k] = json.dumps(v, ensure_ascii=False)
                        elif v is None:
                            pass # Skip None
                        else:
                            processed_data[k] = str(v)
                    
                    if processed_data:
                        client.hset(key, mapping=processed_data)
                        client.expire(key, self.ttl)
                    return
                except Exception as e:
                    logger.error(f"Redis update failed for task {task_id}: {e}")
                    # Fallback to memory? 
                    # Ideally we should stick to one source of truth.
                    # If Redis fails, we might lose state updates.
                    # For now, let's just log error to avoid blocking the process.

        # 2. Memory Fallback (or Primary if Redis disabled)
        if task_id not in self.memory_store:
             self.memory_store[task_id] = {}
        
        # Ensure owner_user_id is set in memory for consistency
        if "owner_user_id" not in self.memory_store[task_id] and user_id:
            self.memory_store[task_id]["owner_user_id"] = user_id
            
        self.memory_store[task_id].update(data)

    def get(self, task_id, user_id):
        """
        Retrieve task status.
        user_id: required to find the key in Redis mode.
        """
        # 1. Try Redis
        if self.redis_enabled:
            client = self._get_redis()
            if client:
                try:
                    key = self._get_key(user_id, task_id)
                    data = client.hgetall(key)
                    if data:
                        # Deserialize
                        result = {}
                        for k, v in data.items():
                            if k in ['structure', 'result']: # Fields known to be JSON
                                try:
                                    result[k] = json.loads(v)
                                except:
                                    result[k] = v
                            elif k in ['progress', 'owner_user_id']:
                                try:
                                    result[k] = int(v)
                                except:
                                    result[k] = v
                            else:
                                result[k] = v
                        return result
                    else:
                        return None # Not found
                except Exception as e:
                    logger.error(f"Redis get failed for task {task_id}: {e}")
        
        # 2. Memory Fallback
        return self.memory_store.get(task_id)

    def set_initial(self, task_id, data, user_id):
        """Initialize task data (clears previous if any)"""
        if self.redis_enabled:
            client = self._get_redis()
            if client:
                try:
                    key = self._get_key(user_id, task_id)
                    client.delete(key) # Clear old
                except:
                    pass
        
        if task_id in self.memory_store:
            del self.memory_store[task_id]
            
        self.update(task_id, data, user_id)

# Initialize Manager
task_manager = TaskStatusManager()

# å¹¶å‘æ§åˆ¶ï¼šé™åˆ¶åŒæ—¶è¿›è¡Œçš„æ–‡æ¡£å¤„ç†ä»»åŠ¡æ•°é‡
# æœåŠ¡å™¨é…ç½®ï¼š251GB å†…å­˜ï¼Œ80 æ ¸ CPUã€‚
# å³ä½¿é…ç½®å¾ˆé«˜ï¼Œä¸ºäº†é˜²æ­¢æç«¯å¹¶å‘å¯¼è‡´ OOMï¼Œè®¾ç½®ä¸€ä¸ªå®‰å…¨ä¸Šé™ã€‚
# å‡è®¾æ¯ä¸ªå¤§æ–‡ä»¶å¤„ç†æ¶ˆè€— 2-4GB å†…å­˜ï¼Œ20 ä¸ªå¹¶å‘çº¦å ç”¨ 40-80GBï¼Œéå¸¸å®‰å…¨ã€‚
MAX_CONCURRENT_TASKS = 20
task_semaphore = threading.Semaphore(MAX_CONCURRENT_TASKS)

def background_process_wrapper(task_id: str, type_name: str, report_name: str, file_path: str, user_id: int):
    """åå°ä»»åŠ¡åŒ…è£…å™¨ï¼Œç”¨äºæ›´æ–°ä»»åŠ¡çŠ¶æ€å¹¶æ‰§è¡Œå¤„ç†"""
    acquired = False
    
    # å®šä¹‰è¿›åº¦å›è°ƒå‡½æ•°
    def update_progress(percent: int, msg: str):
        # ä½¿ç”¨ task_manager æ›´æ–°çŠ¶æ€
        task_manager.update(task_id, {
            "progress": percent,
            "message": msg
        }, user_id)

    try:
        # å°è¯•è·å–ä¿¡å·é‡ï¼Œå¦‚æœæ»¡äº†åˆ™ç­‰å¾…
        logger.info(f"â³ [ä»»åŠ¡ç­‰å¾…] ID: {task_id} æ­£åœ¨ç­‰å¾…æ‰§è¡Œæ§½ä½ (å½“å‰å¹¶å‘é™åˆ¶: {MAX_CONCURRENT_TASKS})...")
        task_manager.update(task_id, {
            "status": "queued", 
            "message": "æ­£åœ¨æ’é˜Ÿç­‰å¾…å¤„ç†èµ„æº...", 
            "progress": 5
        }, user_id)
        
        task_semaphore.acquire()
        acquired = True
        
        logger.info(f"â–¶ï¸ [ä»»åŠ¡å¼€å§‹] ID: {task_id} è·å–åˆ°æ‰§è¡Œæ§½ä½")
        task_manager.update(task_id, {
            "status": "processing", 
            "message": "æ­£åœ¨åå°å¤„ç†ä¸­...", 
            "progress": 10
        }, user_id)

        # 1. åå°æ‰«ææ–‡æ¡£ç»“æ„ (ä¼˜åŒ–å“åº”é€Ÿåº¦)
        try:
            logger.info(f"ğŸ“‘ [åå°ä»»åŠ¡] ID: {task_id} å¼€å§‹æ‰«ææ–‡æ¡£ç»“æ„...")
            doc_structure = scan_docx_structure(file_path)
            # æ›´æ–°çŠ¶æ€ä¸­çš„ç»“æ„ä¿¡æ¯ï¼Œä¾›å‰ç«¯è½®è¯¢è·å–
            task_manager.update(task_id, {
                "structure": doc_structure,
                "progress": 20
            }, user_id)
            logger.info(f"ğŸ“‘ [åå°ä»»åŠ¡] ID: {task_id} ç»“æ„æ‰«æå®Œæˆï¼Œå…± {len(doc_structure)} ç« èŠ‚")
        except Exception as e:
            logger.warning(f"âš ï¸ [åå°ä»»åŠ¡] ID: {task_id} ç»“æ„æ‰«æå¤±è´¥: {e}")
        
        # è°ƒç”¨æ ¸å¿ƒå¤„ç†é€»è¾‘ï¼Œä¼ å…¥å›è°ƒå’Œ user_id
        is_success, result_msg = process_document(type_name, report_name, file_path, progress_callback=update_progress, user_id=user_id)
        
        if is_success:
            task_manager.update(task_id, {
                "status": "success", 
                "message": result_msg, 
                "progress": 100,
                "result": {
                    "report_generation_status": 0,
                    "report_generation_condition": result_msg,
                    "reportName": report_name,
                    "reportType": type_name,
                    "task_id": task_id
                }
            }, user_id)
            logger.info(f"âœ… [å¼‚æ­¥ä»»åŠ¡å®Œæˆ] ID: {task_id} {result_msg}")
        else:
            task_manager.update(task_id, {
                "status": "failed", 
                "message": f"å¯¼å…¥å¤±è´¥ï¼š{result_msg}", 
                "progress": 100
            }, user_id)
            logger.warning(f"âš ï¸ [å¼‚æ­¥ä»»åŠ¡å¤±è´¥] ID: {task_id} {result_msg}")
            
    except Exception as e:
        logger.error(f"âŒ [å¼‚æ­¥ä»»åŠ¡å¼‚å¸¸] ID: {task_id} {e}", exc_info=True)
        
        error_msg = str(e)
        user_friendly_msg = f"ç³»ç»Ÿå¤„ç†å¼‚å¸¸: {error_msg}"
        error_code = "UNKNOWN_ERROR"
        
        if "There is no item named" in error_msg and "in the archive" in error_msg:
             user_friendly_msg = "æ–‡ä»¶ä¼¼ä¹å·²æŸåï¼Œå†…éƒ¨ç»“æ„ç¼ºå¤±ï¼Œè¯·å°è¯•ä¿®å¤æ–‡æ¡£æˆ–é‡æ–°ä¿å­˜åå†ä¸Šä¼ ã€‚"
             error_code = "DOCX_CORRUPTED"
             logger.info(f"â„¹ï¸ [é”™è¯¯ä¿¡æ¯è½¬æ¢] å°†åŸå§‹é”™è¯¯è½¬æ¢ä¸ºå‹å¥½æç¤º: {user_friendly_msg}")
        elif "BadZipFile" in error_msg or "zipfile" in str(type(e)).lower():
             user_friendly_msg = "æ–‡ä»¶æ ¼å¼é”™è¯¯æˆ–å·²æŸåï¼Œæ— æ³•è§£æã€‚è¯·ç¡®è®¤æ–‡ä»¶æ˜¯å¦ä¸ºæœ‰æ•ˆçš„ .docx æ–‡æ¡£ã€‚"
             error_code = "DOCX_CORRUPTED"
        
        task_manager.update(task_id, {
            "status": "error", 
            "message": user_friendly_msg, 
            "progress": 100,
            "error_code": error_code
        }, user_id)
    finally:
        if acquired:
            task_semaphore.release()
            logger.info(f"â¹ï¸ [ä»»åŠ¡é‡Šæ”¾] ID: {task_id} é‡Šæ”¾æ‰§è¡Œæ§½ä½")
            
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(file_path):
            try:
                os.remove(file_path)
                logger.info(f"ğŸ§¹ å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {file_path}")
            except Exception as cleanup_error:
                logger.error(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {cleanup_error}")

@router.get("/check_import_status/{task_id}")
def check_import_status(task_id: str, current_user: CurrentUser = Depends(require_user)):
    """æŸ¥è¯¢å¯¼å…¥ä»»åŠ¡çŠ¶æ€ (éœ€ç™»å½•ï¼Œä¸”åªèƒ½æŸ¥è‡ªå·±çš„ä»»åŠ¡)"""
    # ä½¿ç”¨ TaskManager è·å–çŠ¶æ€ (Redis/Memory)
    # æ³¨æ„: Redis key åŒ…å« user_idï¼Œæ‰€ä»¥åªèƒ½æŸ¥è¯¢å½“å‰ç”¨æˆ·çš„ä»»åŠ¡
    status_info = task_manager.get(task_id, current_user.id)
    
    if not status_info:
        return {"status": "unknown", "message": "ä»»åŠ¡ä¸å­˜åœ¨"}
    
    # å†æ¬¡æ ¡éªŒ owner_id (è™½ç„¶ key éš”ç¦»å·²ä¿è¯ï¼Œä½†åŒé‡ä¿é™©)
    owner_id = status_info.get("owner_user_id")
    current_user_id = current_user.id
    if owner_id is not None and str(owner_id) != str(current_user_id):
        logger.warning(f"âš ï¸ [è¶Šæƒè®¿é—®] User {current_user_id} å°è¯•æŸ¥çœ‹ User {owner_id} çš„ä»»åŠ¡ {task_id}")
        return {"status": "unknown", "message": "ä»»åŠ¡ä¸å­˜åœ¨"}
        
    return status_info

@router.post("/Import_Doc/")
async def Import_Doc_endpoint(  # æ”¹ä¸ºasync
    background_tasks: BackgroundTasks,
    task_id: str = Form(...),
    status: int = Form(...),
    agentUserId: int = Form(...),
    type_name: str = Form(...),
    report_name: str = Form(...),
    file: UploadFile = File(...),
    current_user: CurrentUser = Depends(require_user)
):
    # ä¼˜å…ˆä½¿ç”¨ Token ä¸­çš„ç”¨æˆ· ID
    user_id = current_user.id
    logger.info(f'ğŸš€ [ä»»åŠ¡æ¥æ”¶] ID: {task_id}, User: {user_id} (Claimed: {agentUserId}), æŠ¥å‘Š: {report_name}, ç±»å‹: {type_name}, æ¨¡å¼: å¼‚æ­¥å¤„ç†')

    # 1. è·¯å¾„å‡†å¤‡
    current_dir = os.getcwd()
    temp_dir = os.path.join(current_dir, "temp_uploads")
    if not os.path.exists(temp_dir):
        os.makedirs(temp_dir)
    
    # 2. æå–å¹¶éªŒè¯æ–‡ä»¶æ‰©å±•å
    original_filename = file.filename
    file_ext = os.path.splitext(original_filename)[1].lower()
    
    # 3. ä¸¥æ ¼æ ¡éªŒæ–‡ä»¶æ ¼å¼ï¼Œåªæ¥å—.docx
    if file_ext != '.docx':
        return {
            "report_generation_status": 1,
            "report_generation_condition": "ç³»ç»Ÿä»…æ”¯æŒæ ‡å‡† OpenXML æ ¼å¼çš„ .docx æ–‡æ¡£ï¼Œè¯·å‹¿ä½¿ç”¨æ—§ç‰ˆ .doc æˆ–æ‰‹åŠ¨ä¿®æ”¹åç¼€åã€‚",
            "task_id": task_id,
            "error_code": "UNSUPPORTED_FILE_FORMAT"
        }

    unique_filename = f"{uuid.uuid4()}{file_ext}"
    temp_file_path = os.path.join(temp_dir, unique_filename)

    try:
        # 4. é‡ç½®æ–‡ä»¶æŒ‡é’ˆï¼Œç¡®ä¿ä»å¼€å¤´è¯»å–
        await file.seek(0)

        # 5. ä¿å­˜æ–‡ä»¶ (ä½¿ç”¨åˆ†å—å†™å…¥ä»¥æ”¯æŒå¤§æ–‡ä»¶)
        with open(temp_file_path, "wb") as buffer:
            # ä½¿ç”¨åˆ†å—å†™å…¥é¿å…å¤§æ–‡ä»¶å†…å­˜æº¢å‡º
            chunk_size = 8192
            while True:
                chunk = await file.read(chunk_size)
                if not chunk:
                    break
                buffer.write(chunk)
        
        # 6. å¼ºåˆ¶å°†æ•°æ®åˆ·å…¥ç£ç›˜ï¼ˆè§£å†³IOç«äº‰é—®é¢˜ï¼‰
        with open(temp_file_path, "ab") as buffer:
            buffer.flush()
            os.fsync(buffer.fileno())
        
        file_size = os.path.getsize(temp_file_path)
        logger.info(f"ğŸ“‚ æ–‡ä»¶å·²æ¥æ”¶: {temp_file_path}, å¤§å°: {file_size / 1024 / 1024:.2f} MB")

        if file_size == 0:
            os.remove(temp_file_path)
            return {"report_generation_status": 1, "report_generation_condition": "æ–‡ä»¶å¤§å°ä¸º0", "task_id": task_id}
        
        # 7. ä¸¥æ ¼çš„æ–‡ä»¶æ ¼å¼æ ¡éªŒ - åœ¨å¼ºåˆ¶åˆ·ç›˜åè¿›è¡Œ
        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆçš„ Zip æ–‡ä»¶ (docx æœ¬è´¨æ˜¯ zip)
        if not zipfile.is_zipfile(temp_file_path):
             logger.warning(f"âš ï¸ æ–‡ä»¶æ ¼å¼æ ¡éªŒå¤±è´¥: {temp_file_path} ä¸æ˜¯æœ‰æ•ˆçš„ zip/docx")
             os.remove(temp_file_path)
             return {
                 "report_generation_status": 1,
                 "report_generation_condition": "æ–‡ä»¶å·²æŸåæˆ–ä¸æ˜¯æœ‰æ•ˆçš„ Word (.docx) æ–‡æ¡£",
                 "task_id": task_id,
                 "error_code": "DOCX_CORRUPTED_OR_INVALID"
             }
        
        # 8. è¿›ä¸€æ­¥éªŒè¯æ˜¯å¦åŒ…å«docxå¿…è¦ç»“æ„
        try:
            with zipfile.ZipFile(temp_file_path, 'r') as zip_file:
                # æ£€æŸ¥æ˜¯å¦å­˜åœ¨å¿…è¦çš„docxæ–‡ä»¶
                required_files = ['word/document.xml', '[Content_Types].xml', 'word/_rels/document.xml.rels']
                missing_files = [f for f in required_files if f not in zip_file.namelist()]
                
                if missing_files:
                    logger.warning(f"âš ï¸ docxæ–‡ä»¶ç¼ºå°‘å¿…è¦ç»„ä»¶: {missing_files}")
                    os.remove(temp_file_path)
                    return {
                        "report_generation_status": 1,
                        "report_generation_condition": "æ–‡æ¡£ç»“æ„ä¸å®Œæ•´ï¼Œå¯èƒ½å·²æŸå",
                        "task_id": task_id,
                        "error_code": "DOCX_STRUCTURE_INCOMPLETE"
                    }
        except zipfile.BadZipFile:
            logger.warning(f"âš ï¸ æ–‡ä»¶æ ¼å¼æ ¡éªŒå¤±è´¥: {temp_file_path} æ— æ³•æ‰“å¼€ä¸ºzipæ–‡ä»¶")
            os.remove(temp_file_path)
            return {
                "report_generation_status": 1,
                "report_generation_condition": "æ–‡ä»¶å·²æŸåæˆ–ä¸æ˜¯æœ‰æ•ˆçš„ Word (.docx) æ–‡æ¡£",
                "task_id": task_id,
                "error_code": "DOCX_CORRUPTED_OR_INVALID"
            }

        # 9. åˆå§‹åŒ–ä»»åŠ¡çŠ¶æ€ (è®°å½• owner_user_id)
        task_manager.set_initial(task_id, {
            "status": "pending",
            "message": "å·²è¿›å…¥å¤„ç†é˜Ÿåˆ—",
            "progress": 0,
            "owner_user_id": user_id
        }, user_id)

        # 10. æäº¤åå°ä»»åŠ¡ (ç«‹å³å“åº”å‰ç«¯)
        # ä¼ å…¥ user_id
        background_tasks.add_task(background_process_wrapper, task_id, type_name, report_name, temp_file_path, user_id)

        # 11. ç«‹å³è¿”å›
        return {
            "report_generation_status": 0,
            "report_generation_condition": "æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼Œæ­£åœ¨åå°å¤„ç†ä¸­ï¼Œè¯·é€šè¿‡ /check_import_status æŸ¥è¯¢è¿›åº¦",
            "status": status,
            "task_id": task_id,
            "mode": "async"
        }

    except Exception as e:
        logger.error(f"âŒ æ¥æ”¶æ–‡ä»¶å¼‚å¸¸: {e}", exc_info=True)
        if os.path.exists(temp_file_path):
            try:
                os.remove(temp_file_path)
            except:
                pass  # å¦‚æœæ¸…ç†å¤±è´¥ï¼Œå¿½ç•¥
        return {
            "report_generation_status": 1,
            "report_generation_condition": f"æ¥æ”¶å¼‚å¸¸: {str(e)}",
            "task_id": task_id
        }

@router.get("/health")
def health_check():
    """ç®€å•çš„å¥åº·æ£€æŸ¥"""
    return {"status": "healthy"}