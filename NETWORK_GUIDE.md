# 网络配置与环境访问指南 (Network Configuration Guide)

本文档旨在说明后端服务 (`generate_report_test`) 在不同环境（本地开发、Docker 容器、VPN 远程连接）下的网络配置策略，特别是关于 AI 模型接口的访问问题。

## 1. 核心网络拓扑

系统主要涉及三个网络节点：

1.  **用户/客户端 (Client)**: 你的电脑（通过 VPN 连接）。
2.  **应用服务器 (App Server)**: 运行 Python 后端服务的机器（宿主机或 Docker 容器）。
3.  **AI 模型服务器 (AI Server)**: 运行 Ollama/vLLM 等模型服务的机器（IP: `192.168.3.10`）。

### 访问链路说明

*   **前端访问**: 客户端 -> VPN -> 应用服务器 (Web API)。
    *   *现象*: 你在浏览器访问 `http://127.0.0.1:34522` (假设 VPN 映射) 或 `http://192.168.x.x:34522`。
*   **后端访问**: 应用服务器 -> 局域网 -> AI 模型服务器。
    *   *关键*: **这是服务器对服务器的请求**。

## 2. 为什么 Docker 容器内不能用 localhost？

当后端服务运行在 Docker 容器中时：
*   `localhost` (127.0.0.1) 指的是 **容器本身**。
*   如果 AI 服务运行在 **宿主机** 或 **局域网其他机器** 上，容器访问 `localhost` 是找不到该服务的。

因此，配置必须使用 **容器可达的真实 IP 地址**（如 `192.168.3.10`），而不是 `localhost`。

## 3. 配置策略 (分流政策)

为了兼容不同环境，我们采用以下配置策略：

### A. 环境变量配置 (.env) - 推荐
在 `.env` 文件中，我们将默认地址设置为局域网真实 IP，这样无论是宿主机直接运行还是 Docker 运行都能互通。

```ini
# .env 文件
# ✅ 推荐：使用局域网 IP，Docker 和宿主机都能访问
AI_BASE_URL=http://192.168.3.10:8005/v1

# ❌ 不推荐：仅限本地非 Docker 环境调试
# AI_BASE_URL=http://localhost:8005/v1
```

### B. 自动回退机制 (代码层)
我们在代码中实现了智能回退机制 (`utils/lyf/base_prompt_ai.py`)：
1.  优先读取 `.env` 中的 `AI_BASE_URL`。
2.  如果连接失败或未配置，自动尝试从数据库 (`llm_config` 表) 读取配置。
3.  数据库配置通常存储的是生产环境可用的稳定地址。

## 4. VPN 用户特别说明

如果你是通过 VPN 连接到开发环境：

1.  **你的浏览器 (Client)**: 
    *   可以通过 `127.0.0.1` (如果 VPN 做了端口转发) 或 VPN 内网 IP 访问后端 Web 页面。
    *   *这对后端配置没有影响*。

2.  **后端服务 (Server)**:
    *   它不走你的 VPN。它直接通过机房/局域网网络访问 AI 服务器。
    *   所以后端配置 **必须填局域网 IP (192.168.3.10)**，而不能填你本地看到的 127.0.0.1。

## 5. 故障排查

如果出现 `Connection error` 或 AI 响应超时：

1.  **检查后端日志**:
    ```bash
    tail -f generate_report_test/logs/test_report.log
    ```
2.  **验证连通性 (在服务器上执行)**:
    ```bash
    # 测试能否访问 AI 服务
    curl http://192.168.3.10:8005/v1/models
    ```
    如果 curl 通但代码不通，检查 Docker 网络模式（推荐使用 `network_mode: "host"` 或确保路由正确）。
