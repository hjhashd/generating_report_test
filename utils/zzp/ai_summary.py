import os
import sys
import json
import logging
from sqlalchemy import create_engine, text
from urllib.parse import quote_plus
from cryptography.fernet import Fernet

# LangChain ç›¸å…³åº“
from langchain_ollama import ChatOllama
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage

# ==========================================
# 0. åŸºç¡€é…ç½®
# ==========================================
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if project_root not in sys.path:
    sys.path.append(project_root)
from zzp import sql_config as config

# ğŸ” å¯†é’¥ (ä¿æŒä¸åŸé¡¹ç›®ä¸€è‡´)
ENCRYPTION_KEY = b'8P_Gk9wz9qKj-4t8z9qKj-4t8z9qKj-4t8z9qKj-4t8=' 
cipher_suite = Fernet(ENCRYPTION_KEY)
logger = logging.getLogger(__name__)

# ==========================================
# 1. å·¥å…·å‡½æ•° (å¤ç”¨åŸé€»è¾‘)
# ==========================================
def get_db_connection():
    encoded_password = quote_plus(config.password)
    db_url = f"mysql+pymysql://{config.username}:{encoded_password}@{config.host}:{config.port}/{config.database}"
    return create_engine(db_url)

def decrypt_text(encrypted_str):
    if not encrypted_str: return None
    try:
        return cipher_suite.decrypt(encrypted_str.encode()).decode()
    except Exception:
        return ""

def get_llm_config_by_id(model_id):
    """æ ¹æ®IDä»æ•°æ®åº“è·å–æ¨¡å‹é…ç½®"""
    engine = get_db_connection()
    sql = text("SELECT llm_type, model_name, api_key, base_url FROM llm_config WHERE id = :id")
    try:
        with engine.connect() as conn:
            result = conn.execute(sql, {"id": model_id}).fetchone()
            if result:
                llm_type, model_name, encrypted_key, base_url = result
                api_key = decrypt_text(encrypted_key) if encrypted_key else ""
                return {
                    "llm_type": llm_type, "model_name": model_name,
                    "api_key": api_key, "base_url": base_url
                }
    except Exception as e:
        logger.error(f"è¯»å–é…ç½®å¤±è´¥: {e}")
    return None

def init_llm_instance(config_data):
    """åˆå§‹åŒ– LLM å®ä¾‹"""
    if not config_data: raise ValueError("é…ç½®æ•°æ®ä¸ºç©º")

    llm_type = config_data['llm_type']
    model_name = config_data['model_name']
    base_url = config_data['base_url']
    api_key = config_data['api_key']

    print(f"ğŸš€ åˆå§‹åŒ–æ€»ç»“æ¨¡å‹: [{llm_type}] {model_name}")
    
    if llm_type == "local":
        return ChatOllama(model=model_name, base_url=base_url, temperature=0.3, num_ctx=8192)
    elif llm_type in ["online", "custom"]:
        return ChatOpenAI(
            api_key=api_key, 
            base_url=base_url, 
            model=model_name, 
            temperature=0.3, # æ€»ç»“ä»»åŠ¡ç¨å¾®å¢åŠ ä¸€ç‚¹ç¡®å®šæ€§
            streaming=True
        )
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {llm_type}")

# ==========================================
# 2. æ ¸å¿ƒæ€»ç»“åŠŸèƒ½å‡½æ•°
# ==========================================

def ai_summary_stream(input_text, model_id, custom_instruction=None, user_id=None):
    """
    å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œ AI æ€»ç»“
    :param input_text: å‰ç«¯ä¼ å…¥çš„å¾…æ€»ç»“æ–‡æœ¬
    :param model_id: æ•°æ®åº“ä¸­çš„æ¨¡å‹ ID
    :param custom_instruction: (å¯é€‰) è‡ªå®šä¹‰æ€»ç»“è¦æ±‚ï¼Œå¦‚'æ‰©å†™'ã€'ç¿»è¯‘'ç­‰ï¼Œé»˜è®¤ä¸º'æ€»ç»“'
    :param user_id: (å¯é€‰) å½“å‰æ“ä½œçš„ç”¨æˆ· IDï¼Œç”¨äºæƒé™æ ¡éªŒæˆ–è·å–ç§æœ‰é…ç½®
    """
    
    # 1. éªŒè¯è¾“å…¥
    if not input_text or len(input_text.strip()) == 0:
        yield f"data: {json.dumps({'error': 'Input text is empty'})}\n\n"
        return

    # 2. è·å–æ¨¡å‹é…ç½®
    llm_config = get_llm_config_by_id(model_id)
    if not llm_config:
        yield f"data: {json.dumps({'error': 'Model config not found'})}\n\n"
        return

    # 3. æ„å»º Prompt
    # å¦‚æœæ²¡æœ‰ç‰¹å®šçš„è‡ªå®šä¹‰æŒ‡ä»¤ï¼Œé»˜è®¤ä½¿ç”¨æ€»ç»“æŒ‡ä»¤
    if not custom_instruction:
        custom_instruction = "è¯·å¯¹ä»¥ä¸‹å†…å®¹è¿›è¡Œç²¾ç‚¼çš„æ€»ç»“ï¼Œæå–æ ¸å¿ƒè§‚ç‚¹ï¼Œè¯­è¨€é€šé¡ºã€é€»è¾‘æ¸…æ™°ã€‚"

    system_prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æœ¬åˆ†æä¸æ€»ç»“åŠ©æ‰‹ã€‚
ä»»åŠ¡ç›®æ ‡ï¼š{custom_instruction}
è¦æ±‚ï¼š
1. ä¿æŒå®¢è§‚ï¼Œä¸æ·»åŠ åŸæ–‡ä¸å­˜åœ¨çš„ä¿¡æ¯ã€‚
2. è¾“å‡ºæ ¼å¼ç›´æ¥ä¸ºçº¯æ–‡æœ¬ï¼Œä¸è¦Markdownä»£ç å—åŒ…è£¹ã€‚
"""

    try:
        # 4. åˆå§‹åŒ–æ¨¡å‹
        llm = init_llm_instance(llm_config)
        
        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=f"ã€å¾…å¤„ç†æ–‡æœ¬ã€‘ï¼š\n{input_text}")
        ]

        # 5. æµå¼ç”Ÿæˆ
        full_response = ""
        for chunk in llm.stream(messages):
            text_chunk = chunk.content if hasattr(chunk, 'content') else str(chunk)
            if text_chunk:
                full_response += text_chunk
                # SSE æ ¼å¼è¿”å›
                yield f"data: {json.dumps({'content': text_chunk}, ensure_ascii=False)}\n\n"
        
        # ç»“æŸæ ‡è®°
        yield "data: [DONE]\n\n"
        logger.info("æ€»ç»“ä»»åŠ¡å®Œæˆ")

    except Exception as e:
        logger.error(f"Summary generation error: {e}")
        yield f"data: {json.dumps({'error': str(e)}, ensure_ascii=False)}\n\n"

# ==========================================
# 3. æµ‹è¯•å…¥å£
# ==========================================
if __name__ == "__main__":
    # --- æ¨¡æ‹Ÿå‰ç«¯è¾“å…¥ ---
    TEST_MODEL_ID = 6  # ç¡®ä¿æ•°æ®åº“é‡Œæœ‰è¿™ä¸ªID
    
    # æ¨¡æ‹Ÿä¸€æ®µé•¿æ–‡æœ¬
    TEST_TEXT = """
    è¿‘å¹´æ¥ï¼Œéšç€å…¨çƒæ°”å€™å˜åŒ–é—®é¢˜æ—¥ç›Šä¸¥å³»ï¼Œå„å›½çº·çº·æå‡ºäº†ç¢³è¾¾å³°ã€ç¢³ä¸­å’Œçš„ç›®æ ‡ã€‚èŠ‚èƒ½é™ç¢³ä¸ä»…æ˜¯åº”å¯¹æ°”å€™å˜åŒ–çš„å¿…ç„¶é€‰æ‹©ï¼Œä¹Ÿæ˜¯æ¨åŠ¨ç»æµé«˜è´¨é‡å‘å±•çš„å†…åœ¨è¦æ±‚ã€‚
    æˆ‘ä»¬éœ€è¦åœ¨å·¥ä¸šã€å»ºç­‘ã€äº¤é€šç­‰é‡ç‚¹é¢†åŸŸå®æ–½èŠ‚èƒ½æ”¹é€ ï¼Œæ¨å¹¿ç»¿è‰²ä½ç¢³æŠ€æœ¯ã€‚åŒæ—¶ï¼Œè¦å€¡å¯¼ç»¿è‰²ä½ç¢³çš„ç”Ÿæ´»æ–¹å¼ï¼Œé¼“åŠ±å…¬ä¼—å‚ä¸èŠ‚èƒ½å‡æ’ã€‚
    æ”¿åºœåº”å‡ºå°ç›¸å…³æ”¿ç­–ï¼Œå®Œå–„èƒ½æºä»·æ ¼æœºåˆ¶ï¼ŒåŠ å¤§å¯¹æ–°èƒ½æºäº§ä¸šçš„æ‰¶æŒåŠ›åº¦ã€‚ä¼ä¸šè¦ç§¯ææ‰¿æ‹…ç¤¾ä¼šè´£ä»»ï¼ŒåŠ å¼ºèƒ½æºç®¡ç†ï¼Œé™ä½ç”Ÿäº§è¿‡ç¨‹ä¸­çš„ç¢³æ’æ”¾ã€‚
    é€šè¿‡å…¨ç¤¾ä¼šçš„å…±åŒåŠªåŠ›ï¼Œæˆ‘ä»¬ä¸€å®šèƒ½å¤Ÿå®ç°èŠ‚èƒ½é™ç¢³çš„ç›®æ ‡ï¼Œå»ºè®¾ç¾ä¸½å®¶å›­ã€‚
    """
    
    print(f"ğŸ“ å¾…æ€»ç»“å­—æ•°: {len(TEST_TEXT)}")
    print("-" * 30)

    # è°ƒç”¨ç”Ÿæˆå™¨
    generator = ai_summary_stream(TEST_TEXT, TEST_MODEL_ID)

    print("ğŸ¤– AI æ­£åœ¨æ€»ç»“ä¸­...\n")
    final_result = ""
    
    for event in generator:
        if "[DONE]" in event:
            break
        
        try:
            # è§£æ SSE æ•°æ®
            json_str = event.replace("data: ", "").strip()
            data = json.loads(json_str)
            
            if "content" in data:
                chunk = data["content"]
                print(chunk, end="", flush=True)
                final_result += chunk
            
            if "error" in data:
                print(f"\nâŒ Error: {data['error']}")
        except Exception as e:
            pass

    print("\n\nâœ… ç»“æŸ")