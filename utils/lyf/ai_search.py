import os
import sys
import json
import time
import logging
# å¼‚æ­¥åº“
import asyncio
import httpx
from bs4 import BeautifulSoup
from datetime import datetime
from typing import Dict, List, AsyncGenerator

from langchain_openai import ChatOpenAI
# ä¿®å¤ç‚¹ 1ï¼šç¡®ä¿å¯¼å…¥ ChatOllama ä»¥é…åˆ fallback å‡½æ•°
from langchain_ollama import ChatOllama 
from langchain_core.messages import (
    SystemMessage,
    HumanMessage,
    AIMessage,
    ToolMessage,
)
from utils.chat_session_manager import ChatSessionManager

# =========================
# é¡¹ç›®è·¯å¾„ & æ—¥å¿—
# =========================
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if project_root not in sys.path:
    sys.path.append(project_root)

logger = logging.getLogger(__name__)
# logging.basicConfig(level=logging.INFO) # ç§»é™¤æ­¤è¡Œï¼Œç”±ä¸»ç¨‹åºç»Ÿä¸€é…ç½®

# =========================
# å…¨å±€æ¨¡å‹å®ä¾‹å’ŒçŠ¶æ€
# =========================
ONLINE_LLM = None
LOCAL_LLM = None

# è®°å½•å½“å‰åœ¨çº¿æ¨¡å‹çš„é…ç½®ï¼Œç”¨äºæ£€æµ‹æ˜¯å¦éœ€è¦é‡æ–°åˆå§‹åŒ–
CURRENT_ONLINE_CONFIG = {
    "model_name": None,
    "base_url": None,
    "api_key": None
}

MODEL_STATUS = {
    "online": "NOT_INIT",
    "local": "NOT_INIT",
}

# =========================
# å…¨å±€ä¼šè¯ï¼ˆRedis + Memoryï¼‰
# =========================
# Initialize Manager with 'chat:search' session type
session_manager = ChatSessionManager(session_type="chat:search")

# =========================
# å¼ºåˆ¶æœç´¢ System Prompt
# =========================
def build_search_system_prompt() -> str:
    today = datetime.now().strftime("%Y-%m-%d")
    return f"""
ä»Šå¤©æ—¥æœŸï¼š{today}

ä½ æ˜¯ä¸€ä¸ªæ”¿åŠ¡çº§ AI æœç´¢ä¸åˆ†æå¼•æ“ï¼Œå¿…é¡»ä¸¥æ ¼éµå¾ªä»¥ä¸‹æµç¨‹ï¼Œä¸å¾—è·³è¿‡ï¼š

ã€å¼ºåˆ¶ Workflowã€‘
1. æ— è®ºç”¨æˆ·æå‡ºä»€ä¹ˆé—®é¢˜ï¼Œä½ å¿…é¡»é¦–å…ˆè°ƒç”¨ web_searchã€‚
2. åœ¨è°ƒç”¨ web_search ä¹‹å‰ï¼Œä¸¥ç¦è¾“å‡ºä»»ä½•å®è´¨æ€§å›ç­”å†…å®¹ã€‚
3. web_search çš„ arguments ä¸­ï¼Œå¿…é¡»åŒ…å« 1â€“3 ä¸ªå¯æ£€ç´¢å…³é”®è¯ã€‚
4. æœ€ç»ˆå›ç­”åªèƒ½åŸºäºæœç´¢ç»“æœã€‚

ã€ç»“æœçº¦æŸã€‘
- è‹¥æœç´¢ç»“æœä¸ºç©ºï¼Œå¿…é¡»å›å¤ï¼š
  â€œè”ç½‘æœç´¢æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯â€
- è‹¥å­˜åœ¨ç»“æœï¼š
  - è¡Œæ–‡æ­£å¼
  - æ˜ç¡®è¯´æ˜â€œæ ¹æ®è”ç½‘æœç´¢ç»“æœâ€

ç¦æ­¢é—²èŠã€‚
"""

# =========================
# åˆå§‹åŒ–æ¨¡å‹
# =========================
def init_online_llm(model_name: str, base_url: str, api_key: str) -> ChatOpenAI:
    logger.info(f"åˆå§‹åŒ–ã€åœ¨çº¿æœç´¢æ¨¡å‹ã€‘: {model_name}")
    return ChatOpenAI(
        model=model_name,
        base_url=base_url,
        api_key=api_key,
        temperature=0.2,
        streaming=True,
        timeout=120, # å¢åŠ è¶…æ—¶æ—¶é—´åˆ° 120 ç§’ï¼Œé€‚åº”æ…¢é€Ÿç½‘ç»œæˆ–å¤æ‚æ€è€ƒ
        max_retries=1, # å‡å°‘é‡è¯•æ¬¡æ•°ï¼Œä»¥ä¾¿å¿«é€Ÿè¿›å…¥ fallback
    )

# ä¿®å¤ç‚¹ 2ï¼šç»Ÿä¸€ä½¿ç”¨ ChatOpenAI ç»“æ„è°ƒç”¨æœ¬åœ° Ollama æ¥å£
def init_local_llm() -> ChatOpenAI:
    logger.warning("âš ï¸ åˆ‡æ¢è‡³ã€æœ¬åœ° Ollama æ¨¡å‹ã€‘")
    # æ ¹æ®ç”¨æˆ·è¦æ±‚ï¼Œä½¿ç”¨ qwen3-coder:30b æˆ– llama3.2:3b
    # è¿™é‡Œé»˜è®¤ä¼˜å…ˆä½¿ç”¨æ€§èƒ½æ›´å¼ºçš„ qwen3-coder:30b
    return ChatOpenAI(
        model="qwen3-coder:30b",
        base_url="http://localhost:11434/v1",
        api_key="ollama", 
        temperature=0.2,
        streaming=True,
    )

def should_fallback_to_local(e: Exception) -> bool:
    msg = str(e).lower()
    type_name = type(e).__name__.lower()
    logger.warning(f"æ­£åœ¨æ£€æŸ¥æ˜¯å¦éœ€è¦é™çº§ | å¼‚å¸¸ç±»å‹: {type(e).__name__} | å¼‚å¸¸æ¶ˆæ¯: {msg}")
    
    # åªè¦æ˜¯åœ¨çº¿æ¨¡å‹æˆæƒå¤±è´¥(401)ã€ä½™é¢ä¸è¶³(402)ã€Keyæ— æ•ˆã€æˆ–é¢åº¦è¶…é™ã€æ¨¡å‹æœªæ‰¾åˆ°(404)ã€è¶…æ—¶æˆ–è¿æ¥å¤±è´¥ï¼Œéƒ½è§¦å‘é™çº§
    reasons = [
        "401", "402", "404", "not found", "incorrect api key", 
        "insufficient balance", "exceeded_current_quota", "authentication",
        "timeout", "connection", "connect", "unreachable", "rate_limit"
    ]
    
    if any(r in msg for r in reasons) or any(r in type_name for r in reasons):
        return True
        
    return False

# =========================
# å¼‚æ­¥åˆå§‹åŒ–æ¨¡å‹
# =========================
async def async_init_online_llm(model_name: str, base_url: str, api_key: str):
    global ONLINE_LLM
    logger.info(f"ğŸ”„ å¼‚æ­¥åˆå§‹åŒ–ã€åœ¨çº¿æœç´¢æ¨¡å‹ã€‘: {model_name}")
    ONLINE_LLM = init_online_llm(model_name, base_url, api_key)
    MODEL_STATUS["online"] = "READY"
    logger.info(f"âœ… ã€åœ¨çº¿æœç´¢æ¨¡å‹ã€‘åˆå§‹åŒ–å®Œæˆ")

async def async_init_local_llm():
    global LOCAL_LLM
    logger.info(f"ğŸ”„ å¼‚æ­¥åˆå§‹åŒ–ã€æœ¬åœ° Ollama æ¨¡å‹ã€‘")
    LOCAL_LLM = init_local_llm()
    # é¢„çƒ­æœ¬åœ°æ¨¡å‹ï¼Œé¿å…é¦–æ¬¡æ¨ç†è€—æ—¶è¿‡é•¿
    try:
        await LOCAL_LLM.ainvoke([HumanMessage(content="Hello")])
        logger.info(f"âœ… ã€æœ¬åœ° Ollama æ¨¡å‹ã€‘é¢„çƒ­å®Œæˆ")
    except Exception as e:
        logger.error(f"âŒ æœ¬åœ°æ¨¡å‹é¢„çƒ­å¤±è´¥: {e}")
    MODEL_STATUS["local"] = "READY"
    logger.info(f"âœ… ã€æœ¬åœ° Ollama æ¨¡å‹ã€‘åˆå§‹åŒ–å®Œæˆ")

# ä¿®å¤ç‚¹ 5ï¼šè§£å†³ init_search_llm_with_fallback çš„é€»è¾‘é‡å¤å’Œç±»å®šä¹‰ä¸ä¸€è‡´
def init_search_llm_with_fallback(
    model_name: str,
    base_url: str,
    api_key: str,
):
    if not api_key or not api_key.strip():
        logger.warning("âš ï¸ æœªæ£€æµ‹åˆ° API Keyï¼Œç›´æ¥ä½¿ç”¨æœ¬åœ°æœç´¢æ¨¡å‹")
        return ChatOllama(
            model="deepseek-r1:32b",
            base_url="http://localhost:11434", # ä¿®å¤ï¼šChatOllama åŸºç¡€åœ°å€ä¸éœ€è¦ /v1
            temperature=0.2,
        )

    try:
        logger.info(f"å°è¯•åˆå§‹åŒ–åœ¨çº¿æ¨¡å‹: {model_name}")
        return ChatOpenAI(
            model=model_name,
            base_url=base_url,
            api_key=api_key,
            temperature=0.2,
            streaming=True,
        )
    except Exception as e:
        logger.error(f"åˆå§‹åŒ–å¤±è´¥ï¼Œé™çº§æœ¬åœ°: {e}")
        return ChatOllama(
            model="deepseek-r1:32b",
            base_url="http://localhost:11434",
            temperature=0.2,
        )

# =========================
# è”ç½‘æœç´¢å®ç° (åŸºäº 360 æœç´¢çˆ¬å– - æ›´é€‚åˆå›½å†…ç¯å¢ƒ)
# =========================
async def web_search(query: str, max_results: int = 5) -> str:
    """
    çœŸå®çš„è”ç½‘æœç´¢å®ç°ã€‚ä½¿ç”¨ 360 æœç´¢ (so.com) å¹¶è§£æç»“æœã€‚
    """
    logger.info(f"ğŸŒ æ­£åœ¨æ‰§è¡ŒçœŸå®è”ç½‘æœç´¢ (360): {query}")
    url = f"https://www.so.com/s?q={query}"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    }
    
    try:
        async with httpx.AsyncClient(timeout=10.0, follow_redirects=True) as client:
            response = await client.get(url, headers=headers)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, "html.parser")
            results = []
            
            # 360 æœç´¢ç»“æœé€šå¸¸åœ¨ li.res-list ä¸­
            result_items = soup.select("li.res-list")
            
            for item in result_items[:max_results]:
                title_tag = item.select_one("h3 a")
                if not title_tag:
                    continue
                    
                title = title_tag.get_text(strip=True)
                # 360 æœ‰æ—¶ä¼šæŠŠçœŸå®é“¾æ¥æ”¾åœ¨ data-url ä¸­
                link = title_tag.get("data-url") or title_tag.get("href", "")
                
                # å°è¯•è·å–æ‘˜è¦
                snippet_tag = item.select_one(".res-desc") or item.select_one(".res-comm-con")
                snippet = snippet_tag.get_text(strip=True) if snippet_tag else "æš‚æ— æ‘˜è¦"
                
                results.append(f"æ ‡é¢˜: {title}\né“¾æ¥: {link}\næ‘˜è¦: {snippet}")
            
            if not results:
                logger.warning("360 æœç´¢æœªè¿”å›æœ‰æ•ˆç»“æœ")
                return "è”ç½‘æœç´¢æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"
                
            return "\n\n".join(results)
            
    except Exception as e:
        logger.error(f"è”ç½‘æœç´¢å¤±è´¥: {e}")
        return f"è”ç½‘æœç´¢é‡åˆ°é”™è¯¯: {str(e)}"


# =========================
# æ ¸å¿ƒæœç´¢ç”Ÿæˆå™¨
# =========================
import json
import logging
from typing import AsyncGenerator
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, ToolMessage

logger = logging.getLogger(__name__)

async def Search_Chat_Generator_Stream(
    user_query: str,
    model_name: str,
    base_url: str,
    api_key: str,
    task_id: str,
) -> AsyncGenerator[str, None]:
    global ONLINE_LLM, LOCAL_LLM, CURRENT_ONLINE_CONFIG
    
    start_time = time.time()
    masked_key = f"{api_key[:6]}******{api_key[-4:]}" if api_key and len(api_key) > 10 else "******"
    logger.info(f"ğŸš€ [AI Search Start] TaskID: {task_id} | Query: {user_query[:100]}... | Model: {model_name}")
    logger.info(f"ğŸ”§ [AI Search Config] BaseURL: {base_url} | API Key: {masked_key}")

    # 1. è‡ªåŠ¨åˆå§‹åŒ–/æ›´æ–°åœ¨çº¿æ¨¡å‹é€»è¾‘
    config_changed = (
        model_name != CURRENT_ONLINE_CONFIG["model_name"] or
        base_url != CURRENT_ONLINE_CONFIG["base_url"] or
        api_key != CURRENT_ONLINE_CONFIG["api_key"]
    )
    
    if ONLINE_LLM is None or config_changed:
        try:
            # å‡è®¾è¯¥å‡½æ•°å·²åœ¨å¤–éƒ¨å®šä¹‰
            await async_init_online_llm(model_name, base_url, api_key)
            CURRENT_ONLINE_CONFIG = {
                "model_name": model_name,
                "base_url": base_url,
                "api_key": api_key
            }
        except Exception as e:
            logger.error(f"åœ¨çº¿æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            MODEL_STATUS["online"] = "ERROR"

    history = session_manager.get_session(task_id)
    if not history:
        history = []

    # 2. åœ¨çº¿çŠ¶æ€é¢„æ£€æŸ¥ (ä¿®æ­£äº† f-string å¼•å·å†²çª)
    if MODEL_STATUS.get("online") != "READY":
         status_val = MODEL_STATUS.get("online", "UNKNOWN")
         msg = f"âŒ åœ¨çº¿æ¨¡å‹æœªå°±ç»ª ({status_val})ï¼Œè¯·æ£€æŸ¥é…ç½®æˆ–ç½‘ç»œè¿æ¥ã€‚"
         err_payload = json.dumps({"content": msg}, ensure_ascii=False)
         yield f"data: {err_payload}\n\n"
         return

# --- ä¿®å¤åçš„å·¥å…·å®šä¹‰ ---
# 3. æ„é€ å·¥å…·é›† (ä¿®æ­£ç‰ˆï¼šå…¼å®¹ LangChain æ ¡éªŒ)
    # is_moonshot = "moonshot" in base_url.lower()
    
    tools = []
    # if is_moonshot:
    #     # ä¼ªè£…å†…ç½®æœç´¢ï¼Œç»•è¿‡ Unsupported function æŠ¥é”™
    #     tools.append({
    #         "type": "function",
    #         "function": {
    #             "name": "$web_search",
    #             "description": "å†…ç½®æœç´¢",
    #             "parameters": {"type": "object", "properties": {}}
    #         }
    #     })
    
    tools.append({
        "type": "function",
        "function": {
            "name": "web_search",
            "description": "è‡ªå®šä¹‰æœç´¢",
            "parameters": {
                "type": "object",
                "properties": {"query": {"type": "string"}},
                "required": ["query"],
            }
        }
    })

    messages = [
        SystemMessage(content=build_search_system_prompt()),
        *history,
        HumanMessage(content=user_query),
    ]

    try:
        # ç¬¬ä¸€é˜¶æ®µï¼šä½¿ç”¨ ainvoke æ¢æµ‹å·¥å…·è°ƒç”¨ (ç¡®ä¿ Kimi å†…ç½®æœç´¢æ¡æ‰‹ç¨³å®š)
        llm_with_tools = ONLINE_LLM.bind_tools(tools)
        response = await llm_with_tools.ainvoke(messages)
        
        # è®°å½•æ˜¯å¦è§¦å‘äº†å·¥å…·
        if response.tool_calls:
            logger.info(f"ğŸ› ï¸ [AI Search Tool] Triggered: {len(response.tool_calls)} tools | TaskID: {task_id}")
            for tc in response.tool_calls:
                logger.info(f"   -> Tool: {tc['name']} | Args: {tc['args']}")
                # --- A è®¡åˆ’ï¼šKimi å†…ç½®æœç´¢åè®® ---
                if tc["name"] == "$web_search":
                    payload = json.dumps({"content": "ğŸŒ æ¿€æ´» Kimi åŸç”Ÿè”ç½‘æœç´¢...\n\n"}, ensure_ascii=False)
                    yield f"data: {payload}\n\n"
                    
                    messages.append(response)
                    # Kimi åè®®æ ¸å¿ƒï¼šToolMessage çš„ content å¿…é¡»æ˜¯åŸå§‹ args çš„ JSON å­—ç¬¦ä¸²
                    messages.append(ToolMessage(
                        content=json.dumps(tc["args"], ensure_ascii=False),
                        tool_call_id=tc["id"]
                    ))
                
                # --- B è®¡åˆ’ï¼šæ‰‹å†™ web_search å…œåº• ---
                elif tc["name"] == "web_search":
                    s_query = tc["args"].get("query", user_query)
                    payload = json.dumps({"content": f"ğŸ” æ­£åœ¨æ‰§è¡Œæ‰‹å†™å¢å¼ºæœç´¢: {s_query}...\n\n"}, ensure_ascii=False)
                    yield f"data: {payload}\n\n"
                    
                    # æ‰§è¡Œæ‚¨åŸæœ‰çš„ web_search å‡½æ•°
                    s_results = await web_search(s_query)
                    logger.info(f"ğŸ“„ [AI Search Result] Length: {len(s_results)} chars | TaskID: {task_id}")
                    messages.append(response)
                    messages.append(ToolMessage(content=s_results, tool_call_id=tc["id"]))

        # ç¬¬äºŒé˜¶æ®µï¼šç”Ÿæˆæœ€ç»ˆæµå¼å›ç­” (ä¿®æ­£äº† f-string åæ–œæ é”™è¯¯)
        logger.info(f"ğŸŒŠ [AI Search Stream] Starting final response generation... | TaskID: {task_id}")
        full_answer = ""
        async for chunk in ONLINE_LLM.astream(messages):
            if chunk.content:
                full_answer += chunk.content
                # ä¿®å¤ï¼šå…ˆ dumps å˜é‡ï¼Œé¿å… yield f-string ä¸­å‡ºç°å¤æ‚è½¬ä¹‰
                chunk_payload = json.dumps({"content": chunk.content}, ensure_ascii=False)
                yield f"data: {chunk_payload}\n\n"
        
        yield "data: [DONE]\n\n"
        
        duration = time.time() - start_time
        logger.info(f"âœ… [AI Search Done] TaskID: {task_id} | Total Time: {duration:.2f}s | Output Length: {len(full_answer)}")

        # æ›´æ–°å¯¹è¯å†å²
        history.append(HumanMessage(content=user_query))
        history.append(AIMessage(content=full_answer))
        session_manager.update_session(task_id, history)

    except Exception as e:
        logger.error(f"âŒ [AI Search Error] TaskID: {task_id} | Error: {str(e)}", exc_info=True)
        # æ„é€ é”™è¯¯æ¶ˆæ¯ payload
        error_str = str(e).lower()
        if "429" in error_str or "rate limit" in error_str or "quota" in error_str:
            err_msg = "âš ï¸ åœ¨çº¿æœåŠ¡ç¹å¿™ï¼ˆ429 Too Many Requestsï¼‰ï¼Œæ­£åœ¨ä¸ºæ‚¨åˆ‡æ¢è‡³å¤‡ç”¨é€šé“æˆ–è¯·ç¨åå†è¯•..."
        elif "401" in error_str or "auth" in error_str:
            err_msg = "âš ï¸ é‰´æƒå¤±è´¥ï¼Œè¯·æ£€æŸ¥ API Key é…ç½®ã€‚"
        elif "timeout" in error_str:
            err_msg = "âš ï¸ ç½‘ç»œè¯·æ±‚è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥ã€‚"
        else:
            err_msg = f"âŒ åœ¨çº¿æœåŠ¡è°ƒç”¨å¤±è´¥: {str(e)}"
            
        err_payload = json.dumps({"content": err_msg}, ensure_ascii=False)
        yield f"data: {err_payload}\n\n"
        yield "data: [DONE]\n\n"

# =========================
# æœ¬åœ°è°ƒè¯• (å·²é€‚é…å¼‚æ­¥)
# =========================
if __name__ == "__main__":
    TEST_TASK_ID = "search_debug_001"
    # --- åœ¨è¿™é‡Œä¿®æ”¹é…ç½® ---
    MODEL_NAME = "kimi-k2-turbo-preview" # æˆ–è€…æ˜¯ä½ æˆªå›¾ä¸­çœ‹åˆ°çš„æ¨¡å‹å
    BASE_URL = "https://api.moonshot.cn/v1"
    API_KEY = "sk-3xjbiepAHiU219dDIemODxQdsBem1aAv2hdDb7HlpWKE908c" 
    async def main_async():
        # åœ¨è¿›ç¨‹å¯åŠ¨æ—¶å¼‚æ­¥åˆå§‹åŒ–æ¨¡å‹
        await asyncio.gather(
            async_init_online_llm(MODEL_NAME, BASE_URL, API_KEY),
            async_init_local_llm()
        )

        # ç­‰å¾…æ¨¡å‹åˆå§‹åŒ–å®Œæˆ
        while MODEL_STATUS["online"] != "READY" and MODEL_STATUS["local"] != "READY":
            await asyncio.sleep(0.1)

        await run_round_async("2026å¹´æˆ‘å›½èŠ‚èƒ½é™ç¢³å·¥ä½œçš„ä¸»è¦æ”¿ç­–è§„åˆ’æ˜¯ä»€ä¹ˆï¼Ÿ")

    async def run_round_async(query: str):
        print("\n" + "=" * 30)
        print(f"ç”¨æˆ·é—®é¢˜: {query}")
        print("=" * 30 + "\n")

        start_time = time.time()
        first_token_time = None
        total_content = ""

        # è·å–å¼‚æ­¥ç”Ÿæˆå™¨
        generator = Search_Chat_Generator_Stream(
            user_query=query,
            model_name=MODEL_NAME,
            base_url=BASE_URL,
            api_key=API_KEY,
            task_id=TEST_TASK_ID,
        )

        # å…³é”®ï¼šä½¿ç”¨ async for éå†å¼‚æ­¥ç”Ÿæˆå™¨
        async for event in generator:
            if "[DONE]" in event:
                break
            if "data:" not in event:
                continue

            try:
                json_str = event.replace("data:", "").strip()
                if not json_str:
                    continue
                payload = json.loads(json_str)

                if first_token_time is None and "content" in payload:
                    first_token_time = time.time()
                    print(f"ğŸ’¡ é¦–å­—è€—æ—¶: {first_token_time - start_time:.2f}s\n" + "-" * 30)

                if "content" in payload:
                    print(payload["content"], end="", flush=True)
                    total_content += payload["content"]

                if "error" in payload:
                    print(f"\nâŒ Error: {payload['error']}")

            except Exception:
                pass

        print("\n\n" + "-" * 30)
        print(f"â±ï¸ æ€»è€—æ—¶: {time.time() - start_time:.2f}s")
        print(f"ğŸ“„ è¾“å‡ºå­—æ•°: {len(total_content)}")

    # ä½¿ç”¨ asyncio.run() æ­£ç¡®åœ°è¿è¡Œå¼‚æ­¥ä¸»å‡½æ•°
    asyncio.run(main_async())
